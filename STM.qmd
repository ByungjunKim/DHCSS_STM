---
title: "STM"
---

# STM

라이브러리 로드

```{r}
library(tidyverse)
library(stm)
library(tm)
library(stminsights)
library(parallel)
library(tidytext)
library(tidylo)
library(stringr)
library(vroom)
library(ggplot2)
```

### 데이터 전처리

데이터 로드

```{r}
tssci <- vroom('./TSSCI_Sociology.csv')
# kci <- vroom('./230313_kci_sociology.csv', delim = ',')
kci_ssci <- vroom('./230313_kci_ssci_sociology.csv')
```

KCI와 TSSCI 데이터 합치기(rbind)

```{r}
# text 컬럼 생성 = title + abstract
tssci$text <- str_c(tssci$title,' ',tssci$abstract)
```

```{r}
# 필요한 컬럼만 선택
tssci <- tssci %>% select(year,text)
```

```{r}
# country 컬럼 추가
tssci$index <- 'tssci'
# kci$index <- 'kci'
```

```{r}
# 컬럼이름 통일
kci_ssci <- kci_ssci %>% rename(year=pub_year)
kci_ssci <- kci_ssci %>% rename(index=country)
```

```{r}
df <- bind_rows(kci_ssci,tssci)
```

```{r}
# 2004~2021 게재된 논문 활용
df <- df %>% filter(year>=2004)
df <- df %>% filter(year<=2021)

# 날짜순 정렬(오름차순)
df <- df %>% arrange(year)
```

모델링전 2개씩 짝지어서 df 생성

```{r}
df_ssci_kci <- df %>% filter(index %in% c('ssci','kci'))
df_ssci_tssci <- df %>% filter(index %in% c('ssci','tssci'))
```

```{r}
# index 명목변수화
df_ssci_kci$index <- as.factor(df_ssci_kci$index)
df_ssci_tssci$index <- as.factor(df_ssci_tssci$index)
```

데이터 기술 통계량 확인

```{r}
summary(df_ssci_kci)
```

```{r}
summary(df_ssci_tssci)
```

### 토크나이징

https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html

불용어

```{r}
custom_stopwords <- c(
'article','study','isbn','press','research','book','pp','eds','chapter','vol','acknowledgements','acknowledgments','paper','bibliography', 'appendix', 'preface', 'references', 'introduction', 'index', 'notes', 'conclusion', 'review','http','et','al','doi',"edited", "volume", "chapters", "editor","editors"
)
```

df_ssci_kci

```{r}
myprocess <- textProcessor(df_ssci_kci$text, metadata = df_ssci_kci,wordLengths = c(2,Inf), lowercase = T, removenumbers = T, removepunctuation = T, removestopwords = T, stem=T, customstopwords = custom_stopwords)
```

```{r}
myprocess
```

삭제된 단어수 확인

```{r}
length(myprocess$docs.removed)
```

```{r}
# N개 이상의 문서에서 등장한 단어만 사용(lower.thresh)
out <- prepDocuments(myprocess$documents, myprocess$vocab, myprocess$meta,lower.thresh = 50)
```

### 모델링

최적 토픽갯수 확인

```{r}
# 오래걸림(리눅스/맥 환경에서만 멀티프로세싱 가능)
model_searchK <- searchK(out$documents, out$vocab, K = c(8:20),
                                prevalence = ~s(year) + index,
                                data = out$meta, init.type="Spectral"
                                  ,cores=detectCores()-1)
saveRDS(model_searchK,'model_searchK.rds')
```

```{r}
plot(model_searchK)
```

```{r}
model_searchK <- readRDS('model_searchK.rds')
model_searchK # 9 (semantic coherence 기준)
```

https://juliasilge.com/blog/evaluating-stm/

```{r}
model_searchK$results %>%
  select(K, exclus, semcoh) %>%
  filter(K %in% c(8:30)) %>%
  unnest(cols = c(K, exclus, semcoh)) %>%
  mutate(K = as.factor(K),KL=paste0('T',as.character(K))) %>% 
  ggplot(aes(semcoh, exclus,label=KL)) +
  geom_point(size = 1, alpha = 0.7) +
  geom_text(check_overlap = T)+
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```

##### 실제 모델링

```{r}
stm_model <- stm(out$documents, out$vocab, K=9,
              prevalence= ~s(year) + index,
              data=out$meta, init.type="Spectral",seed=2023,
              verbose = F)
saveRDS(stm_model,'stm_model.rds')
```

```{r}
summary(stm_model)
```

```{r}
plot(stm_model,type='summary',labeltype = 'frex',n=10)
```

주제별 단어 분포\
참고 : https://bookdown.org/ahn_media/bookdown-demo/anal3topic.html

```{r}
td_beta <- stm_model %>% tidy(matrix = 'beta') 

td_beta %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 7) %>% 
  ungroup() %>% 
  mutate(topic = str_c("주제", topic)) %>% 
  
  ggplot(aes(x = beta, 
             y = reorder(term, beta),
             fill = topic)) +
  geom_col(show.legend = F) +
  facet_wrap(~topic, scales = "free") +
  labs(x = expression("단어 확률분포: "~beta), y = NULL,
       title = "주제별 단어 확률 분포",
       subtitle = "각 주제별로 다른 단어들로 군집") +
  theme(plot.title = element_text(size = 20))
```

주제별 문서 분포

```{r}
td_gamma <- stm_model %>% tidy(matrix = "gamma") 
td_gamma %>% glimpse()
```

```{r}
td_gamma %>% 
  mutate(max = max(gamma),
         min = min(gamma),
         median = median(gamma))
```

```{r}
td_gamma %>% 
  ggplot(aes(x = gamma, fill = as.factor(topic))) +
  geom_histogram(bins = 100, show.legend = F) +
  facet_wrap(~topic) + 
  labs(title = "주제별 문서 확률 분포",
       y = "문서(기사)의 수", x = expression("문서 확률분포: "~(gamma))) +
  theme(plot.title = element_text(size = 20))
```

##### 모델 효과 추정

https://bookdown.org/ahn_media/bookdown-demo/anal4topic.html#%EC%A3%BC%EC%A0%9C-%EB%AA%85%EB%AA%85%EA%B3%BC-%EA%B3%B5%EB%B3%80%EC%9D%B8-%EC%A3%BC%EC%A0%9C%EB%AA%A8%ED%98%95

```{r}
m1_K <- stm_model$settings$dim$K
stm_effect_model <-  estimateEffect(1:m1_K ~s(year)+index,
                                 stm_model, meta = out$meta, uncertainty = "Global")
saveRDS(stm_effect_model,'stm_effect_model.rds')
```

```{r}
summary(stm_effect_model, topics= 1:m1_K)
```

```{r}
# 명목변수 효과 시각화
plot.estimateEffect(stm_effect_model, covariate = "index", 
                    topics = c(1:m1_K), method = "difference",
                    model = stm_model, # to show labels alongside
                    cov.value1 = "kci", cov.value2 = "tssci",
                    xlab = "TSSCI <------------> KCI", xlim = c(-.4, .4),
                    labeltype = "frex", n = 5, 
                    width = 100,  verbose.labels = F)
```

```{r}
# 시계열 시각화(모든 토픽)
plot.estimateEffect(stm_effect_model,model=stm, covariate = "year", 
                    topics = c(1:m1_K), method = "continuous")
```

```{r}
#### 시간에 따른 토픽 비율 변화 (토픽별로)
stm_label<- labelTopics(stm_model, n = 10)
# stm_custom_label <- c('접종순서','거리두기 단계','국내 감염 상황','생활/문화/교육','관련연구/기술',
#                                       '지원정책','관련주','백신 승인','미국 대선','경제 전망','정부/청와대',
#                                       '해외 감염 상황','접종후속대책','변이 바이러스','국제협력','증상/전파','백신/치료제 개발','부작용')

par(mfrow=c(3,3))
j <- 1
for (i in c(1:m1_K))
{
  plot(stm_effect_model, "year", method = "continuous", topics = i, printlegend = F,
  # main = stm_custom_label[j], xaxt = "n")
  #main = paste(paste0('T', i,':'),paste(stm_custom_label[i], collapse = ", "),sep=' '),
  #xaxt ="n")
  
  # 토픽 이름대신 keyword로 표현하고 싶으면 아래 main 활용 
  main =  paste('topic', i,paste(stm_label$frex[i,1:4], collapse = ", "),sep=' '))
  
  yearseq <- seq(from=as.Date('2004-01-01'), to=as.Date('2021-12-31'),by='year')
yearnames <- year(yearseq)
axis(1,at=as.numeric(yearseq) - min(as.numeric(yearseq)),labels=yearnames)
  
  j <- j+1

}
```

```{r}
# 토픽 네트워크
# plot(topicCorr(stm_model),vlabels =stm_custom_label, vertex.label.cex = 0.55)
plot(topicCorr(stm_model), vertex.label.cex = 0.55)
```

stminsights

```{r}
run_stminsights()
```

```{r}
df_gamma <- read_csv('./df_gamma.csv')
df_gamma$index <- as.factor(df_gamma$index)
```

```{r}
summary(df_gamma %>% filter(index=='ssci'))
```

```{r}
boxplot(topic1~index,data=df_gamma)
```

```{r}
boxplot(.~index,data=df_gamma)
```
